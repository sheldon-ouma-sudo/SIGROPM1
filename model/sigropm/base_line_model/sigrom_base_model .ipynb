{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8064dc7-0619-4d3b-9b27-af7e0f17941c",
   "metadata": {},
   "source": [
    "Base Model Outline Objective\n",
    "\n",
    "\n",
    "\n",
    "Train a basic text-to-text model using a pre-trained Hugging Face transformer (e.g., T5-small) on tokenized data to establish a baseline for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc6c13d-81bd-4882-8b2f-7c38ff04f126",
   "metadata": {},
   "source": [
    "We start by installing and importing all the libraries we require for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6295b1-cb27-4629-aa37-b58699fb7ab6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r SIGROPM1/model/sigropm/requirements.txt\n",
    "!pip install -U sagemaker\n",
    "!pip install boto3 awscli --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b958aa6-e0c7-43f2-8803-8965956e72ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core SageMaker libraries\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# For model training and deployment\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# For data preprocessing and handling\n",
    "import boto3  # AWS SDK for Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For managing S3 bucket and files\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857a520-e847-40b1-aa44-6220d462e15d",
   "metadata": {},
   "source": [
    "We will now move on to loading our data. Since it is impossible to upload our data on to github, we will upload the data to S3, and then from there on, we will be using it for our subsequent projects and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972339fd-e2ba-48f7-a886-1665833af30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = \"squad-training-data\"  # Use a valid bucket name\n",
    "region = \"us-west-1\"\n",
    "\n",
    "try:\n",
    "    s3.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={'LocationConstraint': region}\n",
    "    )\n",
    "    print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "except s3.exceptions.BucketAlreadyExists:\n",
    "    print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating bucket: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4537e5-23ce-4126-a012-ad8e59b2eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_path = \"/home/sagemaker-user/SIGROPM1/data/expanded_training_data.jsonl\"\n",
    "s3_data_key = \"datasets/training_data.jsonl\"  # Path in S3\n",
    "\n",
    "try:\n",
    "    s3.upload_file(local_data_path, \"squad-training-data\", s3_data_key)\n",
    "    print(f\"Dataset uploaded to s3://squad-training-data/{s3_data_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b30f2-67fd-4673-861f-b37ba9a5d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install s3fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8927cc-5ae3-4c00-a171-3aa37228f527",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "s3_file_path = \"s3://squad-training-data/datasets/training_data.jsonl\"\n",
    "\n",
    "# Load JSONL file into a pandas DataFrame\n",
    "df = pd.read_json(s3_file_path, lines=True)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff37c9-d697-4598-b3d1-10a2ef3c85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Get the SageMaker execution role\n",
    "sagemaker_role = get_execution_role()\n",
    "\n",
    "print(f\"SageMaker Role: {sagemaker_role}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca03127-ffb4-4d78-a9fa-cb8d35d0e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "# Load JSONL file into a pandas DataFrame\n",
    "df = pd.read_json(s3_file_path, lines=True)\n",
    "print(\"Data Sample:\")\n",
    "print(df.head())\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(\"Dataset Preview:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bedd11-f8c2-4f98-a35c-f67299abdda3",
   "metadata": {},
   "source": [
    "Tokenization proceeds and data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830453b3-9205-43c3-bc8a-d27a4d79e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce37b4-8093-434b-9f02-f2ea37afe878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"Pre-installed PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87832d-9fa7-4af4-bdbb-bd16ac9fb99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point = \"/home/sagemaker-user/SIGROPM1/model/sigropm/train.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76270795-d679-4822-840f-5229b28dc4c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"/home/sagemaker-user/SIGROPM1/data/expanded_training_data.jsonl\"\n",
    "# Load the dataset\n",
    "with open(data_path, \"r\") as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, validation_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the splits locally\n",
    "train_data_path = \"/home/sagemaker-user/SIGROPM1/data/train_data.jsonl\"\n",
    "validation_data_path = \"/home/sagemaker-user/SIGROPM1/data/validation_data.jsonl\"\n",
    "\n",
    "with open(train_data_path, \"w\") as train_file:\n",
    "    for entry in train_data:\n",
    "        json.dump(entry, train_file)\n",
    "        train_file.write(\"\\n\")\n",
    "\n",
    "with open(validation_data_path, \"w\") as validation_file:\n",
    "    for entry in validation_data:\n",
    "        json.dump(entry, validation_file)\n",
    "        validation_file.write(\"\\n\")\n",
    "\n",
    "print(f\"Train data saved to {train_data_path}\")\n",
    "print(f\"Validation data saved to {validation_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c519f73e-e48d-4bc5-aad1-094682654060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the S3 bucket name\n",
    "bucket_name = \"s3-sigrom-model-data-bucket\"\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "try:\n",
    "    s3.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={'LocationConstraint': region}\n",
    "    )\n",
    "    print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "except s3.exceptions.BucketAlreadyExists:\n",
    "    print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating bucket: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d880a5-ac0f-4986-883c-e11ba03ca86b",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Upload the train dataset to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=train_data_path,  # Local path to the train data\n",
    "    Bucket=bucket_name,        # Name of your S3 bucket\n",
    "    Key=train_s3_path,         # Path in the S3 bucket\n",
    ")\n",
    "\n",
    "# Upload the validation dataset to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=validation_data_path,  # Local path to the validation data\n",
    "    Bucket=bucket_name,             # Name of your S3 bucket\n",
    "    Key=validation_s3_path,         # Path in the S3 bucket\n",
    ")\n",
    "\n",
    "# Generate S3 URIs\n",
    "train_s3_uri = f\"s3://{bucket_name}/{train_s3_path}\"\n",
    "validation_s3_uri = f\"s3://{bucket_name}/{validation_s3_path}\"\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Train data uploaded to: {train_s3_uri}\")\n",
    "print(f\"Validation data uploaded to: {validation_s3_uri}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18958fd4-8152-4a80-a6f9-8ecfc2e398a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Define S3 input\n",
    "train_s3_uri = \"s3://squad-training-data/datasets/training_data.jsonl\"\n",
    "train_input = TrainingInput(train_s3_uri, content_type=\"application/jsonlines\")\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"/home/sagemaker-user/SIGROPM1/model/sigropm\",  # Directory containing train.py and requirements.txt\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t3.xlarge\",\n",
    "    framework_version=\"1.12.0\",\n",
    "    py_version=\"py38\",\n",
    "    dependencies=[\"/home/sagemaker-user/SIGROPM1/model/sigropm/requirements.txt\"],  # Ensure requirements.txt is included\n",
    "    hyperparameters={\"epochs\": 5, \"batch_size\": 16},\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535033d2-e380-450f-9db6-0eed41a3e769",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the training job\n",
    "estimator.fit({\"train\": train_input})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
