{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc5cda-4eb2-4e69-b57f-28c80b487e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets sagemaker accelerate evaluate --quiet\n",
    "!pip install -U sagemaker\n",
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc228f-324f-452e-a27e-bde279a4b8f2",
   "metadata": {},
   "source": [
    "2.1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817176b-826b-4a95-9f47-daf0726e8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1b7f5-6422-4a14-b5f6-5abef578aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a6171-d111-4199-9bb1-367b86f49286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket and file details\n",
    "bucket_name = \"squad-training-data\"\n",
    "s3_data_key = \"datasets/training_data.jsonl\"  # Path in S3\n",
    "local_data_dir = \"./data\"\n",
    "local_file_path = os.path.join(local_data_dir, \"training_data.jsonl\")\n",
    "\n",
    "# Create local directory if it doesn't exist\n",
    "os.makedirs(local_data_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42134d-5881-4d7d-80e6-4be5fb67c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3.download_file(bucket_name, s3_data_key, local_file_path)\n",
    "    print(f\"File downloaded successfully to {local_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588f986-89c8-49fd-816c-cdb47d183069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the local JSONL file\n",
    "dataset = load_dataset(\"json\", data_files=local_file_path)\n",
    "\n",
    "# Preview the dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b768b40-1874-40aa-8abf-6f39b75b8f86",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5ab63-0239-425e-90c6-57dccd437b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the dataset from the JSONL file\n",
    "dataset = load_dataset(\"json\", data_files=local_file_path)\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_data(example):\n",
    "    example[\"input_text\"] = example[\"prompt\"]\n",
    "    example[\"target_text\"] = example[\"squad\"]\n",
    "    return example\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_dataset = dataset[\"train\"].map(preprocess_data, remove_columns=[\"prompt\", \"squad\"])\n",
    "\n",
    "# Split dataset into train, validation, and test\n",
    "split_dataset = processed_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"test\": split_dataset[\"test\"]\n",
    "})\n",
    "\n",
    "# Further split test into validation and test sets\n",
    "final_dataset = final_dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"validation\": final_dataset[\"train\"],\n",
    "    \"test\": final_dataset[\"test\"]\n",
    "})\n",
    "\n",
    "# Save processed dataset locally\n",
    "final_dataset.save_to_disk(\"./data/processed_dataset\")\n",
    "\n",
    "# Verify dataset\n",
    "print(final_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c17932-53b9-49ac-800a-a548e0f1fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941510c-5ae5-4a38-a69b-1f5a87ecbf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the preprocessed dataset from disk\n",
    "processed_dataset = load_from_disk(\"./data/processed_dataset\")\n",
    "\n",
    "# Verify the structure\n",
    "print(processed_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662852a8-14ed-44a2-95fb-172a0fa3d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 samples from the training data\n",
    "print(processed_dataset[\"train\"][:5])\n",
    "# Display the first 5 samples from the validation data\n",
    "print(processed_dataset[\"validation\"][:5])\n",
    "# Display the first 5 samples from the test data\n",
    "print(processed_dataset[\"test\"][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fbf655-9ead-4726-998d-76a42ee7693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the preprocessed dataset from disk\n",
    "processed_dataset = load_from_disk(\"./data/processed_dataset\")\n",
    "\n",
    "# Verify the structure\n",
    "print(processed_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b79a95-af27-4e5d-9338-e68ff51985f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 bucket and key\n",
    "bucket_name = \"squad-training-data\"\n",
    "processed_data_s3_key = \"datasets/processed_dataset\"\n",
    "\n",
    "# Upload processed dataset to S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "for root, dirs, files in os.walk(\"./data/processed_dataset\"):\n",
    "    for file in files:\n",
    "        s3.upload_file(\n",
    "            os.path.join(root, file),\n",
    "            bucket_name,\n",
    "            f\"{processed_data_s3_key}/{os.path.relpath(os.path.join(root, file), './data/processed_dataset')}\"\n",
    "        )\n",
    "\n",
    "print(f\"Processed dataset uploaded to s3://{bucket_name}/{processed_data_s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e43c3f-ce18-43fe-adf2-7406435320f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "framework = \"pytorch\"\n",
    "region = \"us-west-1\"  # Specify your AWS region\n",
    "instance_type = \"ml.p3.2xlarge\"\n",
    "py_versions = [\"py39\", \"py38\"]\n",
    "framework_versions = [\"1.13\", \"2.0\", \"2.1\"]  # Add other versions you want to test\n",
    "\n",
    "# Check compatibility\n",
    "for py_version in py_versions:\n",
    "    for framework_version in framework_versions:\n",
    "        try:\n",
    "            uri = retrieve(\n",
    "                framework=framework,\n",
    "                region=region,\n",
    "                version=framework_version,\n",
    "                py_version=py_version,\n",
    "                instance_type=instance_type,\n",
    "                image_scope=\"training\",\n",
    "            )\n",
    "            print(f\"Compatible: PyTorch {framework_version}, Python {py_version} -> {uri}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Not compatible: PyTorch {framework_version}, Python {py_version} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21645ebe-5e67-49ec-8239-f19a268ef3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86fb246-91b5-4cbb-86c8-a081a55dc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c6af6-1976-46d1-ab52-bdc771a65033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "\n",
    "framework = \"pytorch\"\n",
    "transformers_version = \"4.38.2\"\n",
    "pytorch_version = \"2.4.0\"\n",
    "py_version = \"py311\"\n",
    "region = sagemaker.Session().boto_region_name\n",
    "\n",
    "try:\n",
    "    image_uri = image_uris.retrieve(\n",
    "        framework=framework,\n",
    "        region=region,\n",
    "        version=pytorch_version,\n",
    "        py_version=py_version,\n",
    "        image_scope=\"training\",\n",
    "        base_framework_version=transformers_version,\n",
    "    )\n",
    "    print(f\"Compatible SageMaker image: {image_uri}\")\n",
    "except Exception as e:\n",
    "    print(f\"No compatible SageMaker image found: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b479a6-0488-4a19-bea7-cb0c9224ac9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# Define hyperparameters and settings\n",
    "hyperparameters = {\n",
    "    \"epochs\": 3,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"model_name\": \"t5-large\",\n",
    "    \"max_seq_length\": 128\n",
    "}\n",
    "\n",
    "# Set up Hugging Face Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "   entry_point=\"train_t5.py\",  # Script to run\n",
    "    source_dir=\"/home/sagemaker-user/SIGROPM1/model/sigropm/major_model\",\n",
    "    instance_type=\"ml.g4dn.4xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role,  \n",
    "    transformers_version=\"4.28\",\n",
    "    pytorch_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "# Start training job\n",
    "huggingface_estimator.fit({\"train\": f\"s3://{bucket_name}/{s3_data_key}\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf70e9-80ed-4b3e-9c20-aa951d12da6f",
   "metadata": {},
   "source": [
    "Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2de4e-f7c7-4bd0-822a-ff3e5ee1ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "\n",
    "# Load processed test dataset\n",
    "dataset_path = \"./data/processed_dataset\"\n",
    "datasets = load_from_disk(dataset_path)\n",
    "test_dataset = datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1e7de-dc2f-437d-8cbb-5e66db94b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(example):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(example[\"input_text\"], return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    # Generate prediction\n",
    "    outputs = model.generate(input_ids)\n",
    "    \n",
    "    # Decode and save the predicted text\n",
    "    example[\"predicted_text\"] = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return example\n",
    "\n",
    "# Apply the function to the test dataset\n",
    "test_results = test_dataset.map(generate_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa2c97-ef3d-4586-ae7c-5de4d80c2236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "# Prepare predictions and references\n",
    "predictions = [example[\"predicted_text\"] for example in test_results]\n",
    "references = [example[\"target_text\"] for example in test_results]\n",
    "\n",
    "# Compute ROUGE scores\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(\"ROUGE Results:\", results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
